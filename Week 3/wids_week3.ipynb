{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "43a1ca59",
            "metadata": {},
            "source": [
                "# Week 3: Transformers & Modern NLP Coding Tasks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "b3bc5a67",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: transformers in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.57.3)\n",
                        "Requirement already satisfied: datasets in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.4.2)\n",
                        "Requirement already satisfied: sentencepiece in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.1)\n",
                        "Requirement already satisfied: torch in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.1)\n",
                        "Requirement already satisfied: accelerate in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.12.0)\n",
                        "Requirement already satisfied: filelock in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.20.1)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.36.0)\n",
                        "Requirement already satisfied: numpy>=1.17 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
                        "Requirement already satisfied: packaging>=20.0 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
                        "Requirement already satisfied: requests in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
                        "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.22.2)\n",
                        "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.7.0)\n",
                        "Requirement already satisfied: tqdm>=4.27 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
                        "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
                        "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (22.0.0)\n",
                        "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.4.0)\n",
                        "Requirement already satisfied: pandas in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.3)\n",
                        "Requirement already satisfied: httpx<1.0.0 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.28.1)\n",
                        "Requirement already satisfied: xxhash in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.6.0)\n",
                        "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.18)\n",
                        "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
                        "Requirement already satisfied: anyio in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
                        "Requirement already satisfied: certifi in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
                        "Requirement already satisfied: httpcore==1.* in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
                        "Requirement already satisfied: idna in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
                        "Requirement already satisfied: h11>=0.16 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
                        "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
                        "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.6.1)\n",
                        "Requirement already satisfied: jinja2 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
                        "Requirement already satisfied: setuptools in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
                        "Requirement already satisfied: psutil in c:\\users\\bodap\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (6.1.0)\n",
                        "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
                        "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
                        "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
                        "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
                        "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
                        "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
                        "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
                        "Requirement already satisfied: colorama in c:\\users\\bodap\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
                        "Requirement already satisfied: sniffio>=1.1 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
                        "Requirement already satisfied: six>=1.5 in c:\\users\\bodap\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
                    ]
                }
            ],
            "source": [
                "!pip install transformers datasets sentencepiece torch accelerate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "0abec70b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\bodap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
                        "  if not hasattr(np, \"object\"):\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WARNING:tensorflow:From c:\\Users\\bodap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
                "from datasets import load_dataset\n",
                "import sentencepiece as spm\n",
                "import os"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2f950ffa",
            "metadata": {},
            "source": [
                "## 1. Inference Using Transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "5a3c8a88",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
                        "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
                        "Device set to use cpu\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Text: Transformers are amazing and have revolutionized NLP!\n",
                        "Label: POSITIVE, Score: 0.9998\n",
                        "\n",
                        "Text: I am not a fan of waiting in long lines.\n",
                        "Label: NEGATIVE, Score: 0.9210\n",
                        "\n",
                        "Text: The movie was okay, but the ending was disappointing.\n",
                        "Label: NEGATIVE, Score: 0.9991\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "classifier = pipeline(\"sentiment-analysis\")\n",
                "\n",
                "examples = [\n",
                "    \"Transformers are amazing and have revolutionized NLP!\",\n",
                "    \"I am not a fan of waiting in long lines.\",\n",
                "    \"The movie was okay, but the ending was disappointing.\"\n",
                "]\n",
                "\n",
                "results = classifier(examples)\n",
                "\n",
                "for text, result in zip(examples, results):\n",
                "    print(f\"Text: {text}\\nLabel: {result['label']}, Score: {result['score']:.4f}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "48cd2cf6",
            "metadata": {},
            "source": [
                "## 2. Fine-tuning BERT for Text Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "1e532930",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting training...\n",
                        "Training setup complete. Uncomment trainer.train() to execute.\n"
                    ]
                }
            ],
            "source": [
                "dataset = load_dataset(\"imdb\")\n",
                "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(100))\n",
                "small_test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(50))\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
                "\n",
                "def tokenize_function(examples):\n",
                "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
                "\n",
                "tokenized_train = small_train_dataset.map(tokenize_function, batched=True)\n",
                "tokenized_test = small_test_dataset.map(tokenize_function, batched=True)\n",
                "\n",
                "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./results\",\n",
                "    eval_strategy=\"epoch\",\n",
                "    learning_rate=2e-5,\n",
                "    per_device_train_batch_size=8,\n",
                "    per_device_eval_batch_size=8,\n",
                "    num_train_epochs=3,\n",
                "    weight_decay=0.01,\n",
                "    logging_dir='./logs',\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_train,\n",
                "    eval_dataset=tokenized_test,\n",
                ")\n",
                "\n",
                "print(\"Starting training...\")\n",
                "# trainer.train()\n",
                "print(\"Training setup complete. Uncomment trainer.train() to execute.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d218aa5a",
            "metadata": {},
            "source": [
                "## 3. Train a Custom SentencePiece Tokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "c59122db",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Original: SentencePiece is an unsupervised text tokenizer\n",
                        "Pieces: ['▁', 'S', 'en', 't', 'en', 'ce', 'P', 'i', 'e', 'ce', '▁', 'is', '▁an', '▁', 'u', 'n', 's', 'u', 'p', 'er', 'v', 'is', 'e', 'd', '▁', 'te', 'x', 't', '▁', 'tokenizer']\n",
                        "IDs: [3, 39, 22, 5, 22, 37, 26, 38, 4, 37, 3, 14, 18, 3, 16, 15, 6, 16, 30, 13, 45, 14, 4, 7, 3, 20, 32, 5, 3, 31]\n",
                        "Decoded: SentencePiece is an unsupervised text tokenizer\n"
                    ]
                }
            ],
            "source": [
                "dummy_text = \"\"\"\n",
                "Transformers are state-of-the-art models in NLP.\n",
                "SentencePiece is an unsupervised text tokenizer and detokenizer.\n",
                "It is mainly used for Neural Network-based text generation systems.\n",
                "Machine learning is fascinating and powerful.\n",
                "\"\"\"\n",
                "\n",
                "with open(\"bot_corpus.txt\", \"w\") as f:\n",
                "    f.write(dummy_text)\n",
                "\n",
                "spm.SentencePieceTrainer.train(input='bot_corpus.txt', model_prefix='m', vocab_size=50)\n",
                "\n",
                "sp = spm.SentencePieceProcessor()\n",
                "sp.load('m.model')\n",
                "\n",
                "text_to_tokenize = \"SentencePiece is an unsupervised text tokenizer\"\n",
                "encoded_pieces = sp.encode_as_pieces(text_to_tokenize)\n",
                "encoded_ids = sp.encode_as_ids(text_to_tokenize)\n",
                "\n",
                "print(f\"Original: {text_to_tokenize}\")\n",
                "print(f\"Pieces: {encoded_pieces}\")\n",
                "print(f\"IDs: {encoded_ids}\")\n",
                "\n",
                "decoded_text = sp.decode_ids(encoded_ids)\n",
                "print(f\"Decoded: {decoded_text}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f4e5aaef",
            "metadata": {},
            "source": [
                "## 4. Mini-Project: Text Summarization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cpu\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Original Text Length: 907\n",
                        "\n",
                        "Summary:\n",
                        "The Transformer is a deep learning model introduced in 2017 by Google researchers in the paper \"Attention Is All You Need\" It is primarily used in the field of natural language processing (NLP)\n"
                    ]
                }
            ],
            "source": [
                "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
                "\n",
                "long_text = \"\"\"\n",
                "The Transformer is a deep learning model introduced in 2017 by Google researchers in the paper \"Attention Is All You Need\". \n",
                "It is primarily used in the field of natural language processing (NLP). \n",
                "Like recurrebt neural networks (RNNs), transformers are designed to process sequential input data, such as natural language, \n",
                "with applications for tasks such as translation and text summarization. \n",
                "However, unlike RNNs, transformers process the entire input all at once. \n",
                "The attention mechanism provides context for any position in the input sequence. \n",
                "For example, if the input data is a natural language sentence, the transformer does not need to process the beginning of it before the end. \n",
                "The transformer model has replaced RNNs and LSTMs as the model of choice for NLP problems, replacing them with attention mechanisms \n",
                "that allow for parallelization and better handling of long-range dependencies.\n",
                "\"\"\"\n",
                "\n",
                "summary = summarizer(long_text, max_length=60, min_length=30, do_sample=False)\n",
                "\n",
                "print(\"Original Text Length:\", len(long_text))\n",
                "print(\"\\nSummary:\")\n",
                "print(summary[0]['summary_text'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "46d842db",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
